#pragma kernel ComputeLayerNone
#pragma kernel ComputeLayerReLU
#pragma kernel ComputeLayerSigmoid
#pragma kernel ComputeLayerTanh01
#pragma kernel BackPropegateZeroErrorsStart
#pragma kernel BackPropegateNoneSinglePass
#pragma kernel BackPropegateReLuSinglePass
#pragma kernel BackPropegateSigmoidSinglePass
#pragma kernel BackPropegateTanh01SinglePass


#pragma kernel  BackPropegateNoneComputeActivationDerivatiePass
#pragma kernel  BackPropegateReLuComputeActivationDerivatiePass
#pragma kernel  BackPropegateSigmoidComputeActivationDerivatiePass
#pragma kernel  BackPropegateTanh01ComputeActivationDerivatiePass
#pragma kernel  BackPropegateInputPass
#pragma kernel  BackPropegateInput2DPass

// Buffers for weights and biases
RWStructuredBuffer<float> weightBuffer;// each input has a weight to each neuron, so count: numinputs * numNeurons
RWStructuredBuffer<float> biasBuffer;// per neuron
// Buffer for neuron inputs
RWStructuredBuffer<float> inputBuffer;//per input
// Buffer for neuron outputs
RWStructuredBuffer<float> outputBuffer;//per neuron

//***** backpropegation ******
// Buffer for neuron input error backpropegation
RWStructuredBuffer<float> activationDerivativeBuffer;
RWStructuredBuffer<float> progegatedErrorBuffer; //per input
RWStructuredBuffer<float> sourceErrorBuffer; //per neuron
//StructuredBuffer<float> biasErrorBuffer; //per neuron
//StructuredBuffer<float> weightErrorBuffer; //per input per neuron

uniform float learningRate;

// Number of inputs and neurons (passed as uniforms)
uniform uint numInputs;
uniform uint numNeurons;


float ComputeweightedSumSinglePass(uint3 id)//id.x is neuron id
{
    float weightedSum = biasBuffer[id.x];
    int offset = id.x * numInputs;
    for (uint i = 0; i < numInputs; i++)// per input to neuron id.x
    {
        weightedSum += inputBuffer[i] * weightBuffer[offset + i];
    }
    return weightedSum;
}

[numthreads(256, 1, 1)]
void ComputeLayerNone(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= numNeurons)return;
    // ... (No activation function)
    outputBuffer[id.x] = ComputeweightedSumSinglePass(id);
}

[numthreads(256, 1, 1)]
void ComputeLayerReLU(uint3 id : SV_DispatchThreadID)
{
    if (id.x >= numNeurons)return;
    outputBuffer[id.x] = max(0, ComputeweightedSumSinglePass(id));
    // ... (ReLU activation function)
}

[numthreads(256, 1, 1)]
void ComputeLayerSigmoid(uint3 id : SV_DispatchThreadID)
{
    // ... (Sigmoid activation function)
    if (id.x >= numNeurons)return;
    outputBuffer[id.x] = 1.0f / (1.0f + exp(-ComputeweightedSumSinglePass(id)));
}

[numthreads(256, 1, 1)]
void ComputeLayerTanh01(uint3 id : SV_DispatchThreadID)
{
    // ... (Tanh01 activation function)
    if (id.x >= numNeurons)return;
    outputBuffer[id.x] = (1.0f + tanh(-ComputeweightedSumSinglePass(id))) * 0.5f;
}


[numthreads(256, 1, 1)]
void BackPropegateZeroErrorsStart(uint3 id : SV_DispatchThreadID)//id.x is input id
{
    if (id.x >= numInputs)
        return;
    progegatedErrorBuffer[id.x] = 0;
}
/*
void BackPropegateSinglePass(int neuronID, float outputActiovationDerivative)
{

    float sourceError = sourceErrorBuffer[neuronID];
    // Compute the gradient for biases   
    float bError = sourceError * outputActiovationDerivative;
    //biasErrorBuffer[id.x] =bError; // for rendering
    // Update biases
    biasBuffer[neuronID] -= learningRate * bError; // biasErrorBuffer[id.x];
    uint offset = neuronID * numInputs;
    // Compute the gradient for weights, and sum up input errors for backpropegation
    for (uint j = 0; j < numInputs; j++)
    {
        uint weightIndex = offset + j;
        
        float wError = sourceError * outputActiovationDerivative * inputBuffer[j];
        //weightErrorBuffer[offset + j] =wError;// for rendering
        // Accumulate errors to be propagated to the previous layer
       // if (neuronID != 3)
        //progegatedErrorBuffer[j] += 1;//neuronID; //test should add up to 0.1 * num neurons for each element
        progegatedErrorBuffer[j] +=sourceError * outputActiovationDerivative * weightBuffer[weightIndex];  //fails! overwrites, does not sum
        //update weights
        weightBuffer[weightIndex] -= learningRate * wError; //weightErrorBuffer[offset + j];
    }

}

[numthreads(256, 1, 1)]
void BackPropegateNoneSinglePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons) return;
    float lastOutputActivationDerivative = 1;
    BackPropegateSinglePass(id.x, lastOutputActivationDerivative);
}

[numthreads(256, 1, 1)]
void BackPropegateReLuSinglePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons)
        return;
    float lastOutputActivationDerivative = 0;
    if (outputBuffer[id.x] > 0)
        lastOutputActivationDerivative = 1;
    BackPropegateSinglePass(id.x, lastOutputActivationDerivative);

}
[numthreads(256, 1, 1)]
void BackPropegateSigmoidSinglePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons)
        return;
    float sigmoid = 1.0f / (1.0f + exp(-outputBuffer[id.x]));
    float lastOutputActivationDerivative = sigmoid * (1 - sigmoid);
    if (outputBuffer[id.x] > 0)
        lastOutputActivationDerivative = 1;
    BackPropegateSinglePass(id.x, lastOutputActivationDerivative);

}
[numthreads(256, 1, 1)]
void BackPropegateTanh01SinglePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons)
        return;
    float t = (1.0f + (float) tanh(outputBuffer[id.x] * 2.0f - 1.0f)) * 0.5f;
    float lastOutputActivationDerivative = (1 - t) * t; // Derivative of tanh (scaled to [0, 1])
    BackPropegateSinglePass(id.x, lastOutputActivationDerivative);

}

*/
void BackPropegateBiasPass(uint3 id)//id.x is neuron index
{
    uint neuronID = id.x;
    float sourceError = sourceErrorBuffer[neuronID];
    // Compute the gradient for biases   
    float bError = sourceError * activationDerivativeBuffer[neuronID];
    //biasErrorBuffer[id.x] =bError; // for rendering
    // Update biases
    biasBuffer[neuronID] -= learningRate * bError; // biasErrorBuffer[id.x];
}

[numthreads(256, 1, 1)]
void BackPropegateInputPass(uint3 id : SV_DispatchThreadID)//id.x is input index
{
    if (id.x >= numInputs)
        return;

    uint inputIndex = id.x;
    
    // Compute the gradient for weights, and sum up input errors for backpropegation
    for (uint neuronIndex = 0; neuronIndex < numNeurons; neuronIndex++)
    {
        uint weightIndex = neuronIndex * numInputs + inputIndex; //each neuron has numInputs inputs -  (total inputs for layer: numNeurons * numInputs)
        float outputActiovationDerivative = activationDerivativeBuffer[neuronIndex];
        float sourceError = sourceErrorBuffer[neuronIndex];
        float wError = sourceError * outputActiovationDerivative * inputBuffer[inputIndex];
        progegatedErrorBuffer[inputIndex] += sourceError * outputActiovationDerivative * weightBuffer[weightIndex];
        weightBuffer[weightIndex] -= learningRate * wError;
    }

}

[numthreads(256, 1, 1)]
void BackPropegateInput1DPassFAILS(uint3 id : SV_DispatchThreadID)//id.x is weight index
{
    if (id.x >= numInputs * numNeurons)
        return;

    uint weightIndex = id.x;
    uint inputIndex = weightIndex % numInputs;
    uint neuronIndex = weightIndex / numInputs; //drop the fraction/remainder
    float outputActiovationDerivative = activationDerivativeBuffer[neuronIndex];
    float sourceError = sourceErrorBuffer[neuronIndex];
    float wError = sourceError * outputActiovationDerivative * inputBuffer[inputIndex];
    progegatedErrorBuffer[inputIndex] += 0.1;//sourceError * outputActiovationDerivative * weightBuffer[weightIndex];
    weightBuffer[weightIndex] -= learningRate * wError;
}

[numthreads(32, 32, 1)]
void BackPropegateInput2DPass(uint3 id : SV_DispatchThreadID)//id.x is inputIndex index, id.y is neuronIndex
{
    if (id.x >= numInputs || id.y >= numNeurons)
        return;
    uint inputIndex = id.x;
    uint neuronIndex = id.y;
    uint weightIndex = neuronIndex * numInputs + inputIndex;
    float outputActiovationDerivative = activationDerivativeBuffer[neuronIndex];
    float sourceError = sourceErrorBuffer[neuronIndex];
    float wError = sourceError * outputActiovationDerivative * inputBuffer[inputIndex];
    progegatedErrorBuffer[inputIndex] += sourceError * outputActiovationDerivative * weightBuffer[weightIndex];
    weightBuffer[weightIndex] -= learningRate * wError;
}

[numthreads(256, 1, 1)]
void BackPropegateNoneComputeActivationDerivatiePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons)
        return;
    activationDerivativeBuffer[id.x] = 1;
    BackPropegateBiasPass(id.x);

}

[numthreads(256, 1, 1)]
void BackPropegateReLuComputeActivationDerivatiePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons)
        return;
    float lastOutputActivationDerivative = 0;
    if (outputBuffer[id.x] > 0)
        lastOutputActivationDerivative = 1;
    activationDerivativeBuffer[id.x] = lastOutputActivationDerivative;
    BackPropegateBiasPass(id.x);
}
[numthreads(256, 1, 1)]
void BackPropegateSigmoidComputeActivationDerivatiePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons)
        return;
    float sigmoid = 1.0f / (1.0f + exp(-outputBuffer[id.x]));
    float lastOutputActivationDerivative = sigmoid * (1 - sigmoid);
    if (outputBuffer[id.x] > 0)
        lastOutputActivationDerivative = 1;
    activationDerivativeBuffer[id.x] = lastOutputActivationDerivative;
    BackPropegateBiasPass(id.x);
}
[numthreads(256, 1, 1)]
void BackPropegateTanh01ComputeActivationDerivatiePass(uint3 id : SV_DispatchThreadID)//id.x is neuron id
{
    if (id.x >= numNeurons)
        return;
    float t = (1.0f + (float) tanh(outputBuffer[id.x] * 2.0f - 1.0f)) * 0.5f;
    float lastOutputActivationDerivative = (1 - t) * t; // Derivative of tanh (scaled to [0, 1])
    activationDerivativeBuffer[id.x] = lastOutputActivationDerivative;
    BackPropegateBiasPass(id.x);
}
